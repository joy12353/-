
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier,plot_tree
from matplotlib.pyplot import plot

#dataset loading
cancer=load_breast_cancer()
#random_state: ë°ì´í„°ê°€ ì„ì—¬ë„ ìƒê´€ ì—†ìŒ / ë§Œì•½ ì„ì´ë©´ ì ˆëŒ€ ì•ˆëœë‹¤ í•˜ë©´ random_stateì§€ì • í•´ì•¼í•¨ 
X_train,X_test,y_train,y_test=train_test_split(cancer.data,cancer.target,stratify=cancer.target,random_state=42)

tree=DecisionTreeClassifier(random_state=42)    #hyperparameter: default
tree.fit(X_train,y_train)
print('Train score:',tree.score(X_train,y_train))
print('Test_score:',tree.score(X_test,y_test))

tree1=DecisionTreeClassifier(max_depth=10,random_state=42)
tree1.fit(X_train,y_train)
print('tree1score:',tree1.score(X_test,y_test))

#class_names=['[malignant','benign']
#plot_tree(tree,feature_names=class_names,filled=True)
#plt.show()

print('tree feature importances \n',tree.ffeature_importances)
#\n:ê°œí–‰(enter)
#\t" tab í‚¤ 

def plot_feature_importance_plot(model):
    n_features=cancer.data.shape[1] #data: rowXcol-> data[1]->col
    plt.barh(np.arange(n_features),model.feature_importances_,align='center')
    
    #Make a horizontal bar plot
    

tree.score(X_train,y_train) #train ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ ìŠ¤ì½”ì–´ 

#train_test_split(X,y,test_size) y=\->target
#DecisionTreeClassifier í˜¸ì¶œ
dt_clf=DecisionTreeClassifier(min_samples_leaf=6,random_state=42)

#DecisionTree í•™ìŠµ
dt_clf.fit(X_train,y_train)
print(dt_clf.score(X_test,y_test))

#íŠ¸ë¦¬ë¥¼ ê·¸ë ¤ë³´ì 

    #explt_vars=['sepal_length','sepal_width','petal_length','petal_width']
    #fct_val={0:'setosa',1:'versicolor',2:'virginica'}

    #plot.figure(figsize=(10,8))
    #plot_tree(dt_clf, feature_names=-explt_vars,class_names=fct_val,filled=True)
    #plt.show()

import seaborn as sns 
import numpy as np
#feature importance ì¶”ì¶œ 
print('Feature importance:\n{0}'.format(np.round(dt_clf.feature_importances_,3)))

#featureë³„ importance mapping 
for name,value in zip(iris_dataset.feature_names,dt_clf.feature_importances_):
    print('{0}:{1:.3f}'.format(name,value))
          
          
sns.barplot(x=dt_clf.feature_importances_,y=iris_dataset.features_names)
plt.show()

ğŸ˜‰ëœë¤ í¬ë ˆìŠ¤íŠ¸(Random Forest)
ì˜¤ëŠ˜ë‚  ì“°ì´ëŠ” ë¨¸ì‹  ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ì¤‘ì—ì„œ ê°€ì¥ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ìë‘í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜

ëœë¤ í¬ë ˆìŠ¤íŠ¸ëŠ” ì´ ì¤‘ ë°°ê¹… ë°©ë²•ì„ ì ìš©í•œ ê²°ì •íŠ¸ë¦¬(Dicison Tree)ì˜ ì•™ìƒë¸”

ëœë¤ í¬ë ˆìŠ¤íŠ¸ ì•Œê³ ë¦¬ì¦˜ì€ ê²°ì •íŠ¸ë¦¬(Dicision Tree) ë¶„ë¥˜ê¸° ì—¬ëŸ¬ ê°œë¥¼ í›ˆë ¨ì‹œì¼œì„œ ê·¸ ê²°ê³¼ë“¤ì„ ê°€ì§€ê³  ì˜ˆì¸¡ì„ í•¨. ê°ê°ì˜ ê²°ì •íŠ¸ë¦¬ë¥¼ í›ˆë ¨ì‹œí‚¬ ë•Œ ë°°ê¹…(Bagging). ì¦‰, í›ˆë ¨ ì„¸íŠ¸ì—ì„œ ì¤‘ë³µì„ í—ˆìš©í•˜ì—¬ ìƒ˜í”Œë§í•œ ë°ì´í„°ì…‹ì„ ê°œë³„ ê²°ì •íŠ¸ë¦¬ ë¶„ë¥˜ê¸°ì˜ í›ˆë ¨ ì„¸íŠ¸ë¡œ í•˜ì—¬ í›ˆë ¨ì‹œí‚¤ëŠ” ë°©ì‹.

ì¥ì  :
ë¶„ë¥˜, íšŒê·€ì—ì„œ ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì„±ëŠ¥ì´ ì¢‹ê³  ì •í™•ë„ê°€ ë†’ë‹¤
ê°„í¸í•˜ê³  ë¹ ë¥´ë‹¤.
í° ë°ì´í„° ì…‹ì—ì„œë„ ì˜ ì‚¬ìš©ë˜ë©°, ë§ì€ ì…ë ¥ ë³€ìˆ˜ë“¤ì„ ë‹¤ë£° ìˆ˜ ìˆë‹¤.
ë‹¨ì  :
ì†ë„ì™€ ë©”ëª¨ë¦¬ ë¹„ìš©ì´ ìƒëŒ€ì (linearì— ë¹„í•´)ìœ¼ë¡œ í¼ (ë” ë§ì€ íŠ¸ë¦¬ -> ì •í™•ë„ëŠ” ë†’ì•„ì§€ë‚˜ ì‹œê°„ê³¼ ë¦¬ì†ŒìŠ¤ ì†Œëª¨ê°€ í¼)

íŠ¸ë¦¬ ê¹Šì´ì™€ ê°œìˆ˜ ì„¤ì •ì„ ì˜ëª»í•˜ì—¬ ê³¼ì í•©(Overfiting)ë°œìƒ.

ëœë¤ í¬ë ˆìŠ¤íŠ¼íŠ¼ í›ˆë ¨ì„ í†µí•´ êµ¬ì„±í•´ ë†“ì€ ë‹¤ìˆ˜ì˜ ì˜ì‚¬ê²°ì •íŠ¸ë¦¬ë“¤ë¡œë¶€í„° ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì·¨í•©í•´ì„œ ì˜ˆì¸¡í•˜ëŠ” ì•™ìƒë¸” ëª¨ë¸ì´ë¼ê³  í•  ìˆ˜ ìˆìŒ.
ì½”ë“œ ì‹¤ìŠµ
ì°¸ê³  : íŒŒì´ì¬ ì™„ë²½ê°€ì´ë“œ

ë°°ê¹…(Bagging)
í†µê³„í•™ì—ì„œ ì¤‘ë³µí•œ í—ˆìš©í•œ ë¦¬ìƒ˜í”Œë§(resampling)ì„ ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘(boostraping)ì´ë¼ í•˜ê³  ì´ì™€ ë°˜ëŒ€ë¡œ ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•ŠëŠ” ìƒ˜í”Œë§ ë°©ì‹ì„ í˜ì´ìŠ¤íŒ…(pasting)ì´ë¼ê³  í•¨.
ex) 10ë§Œê°œì˜ ë°ì´í„° ì¤‘ 7ë§Œ 5ì²œê°œì˜ ë°ì´í„°ë¥¼ í•™ìŠµë°ì´í„°ë¡œ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •í•˜ë©´

7ë§Œ 5ì²œê°œì˜ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ì—¬ëŸ¬ ë°ì´í„° ì…‹ìœ¼ë¡œ ë‚˜ëˆˆë‹¤ë©´ í•™ìŠµë°ì´í„°ê°€ ë¶€ì¡±í•´ì§€ê³  ì´ë¡œì¸í•´ ëª¨í˜•ë“¤ì´ í¸í–¥íˆ ìƒê¸°ê±°ë‚˜ ë¶„ì‚°ì´ ì»¤ì§€ëŠ” ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆë‹¤.

í•˜ì§€ë§Œ ë°°ê¹…ì˜ ê²½ìš°ëŠ” ë³µì›ì¶”ì¶œ(Resampling)ì„ í•˜ê¸° ë•Œë¬¸ì— ì–´ë–¤ ë°ì´í„°ëŠ” ì—¬ëŸ¬ë²ˆ ë½‘í ìˆ˜ ìˆê³  ì–´ë–¤ ë°ì´í„°ëŠ” í•œë²ˆë„ ë½‘íˆì§€ ì•Šì„ ìˆ˜ë„ ìˆë‹¤ ë•Œë¬¸ì— 75000ê°œì˜ ë°ì´í„°ë¥¼ ê°€ì§€ê³  75000ë²ˆì„ ì¶”ì¶œí•´ë„ 75000ê°€ì§€ì˜ ì„œë¡œ ë‹¤ë¥¸ ìƒ˜í”Œë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆê²Œ ë¨.

ì´ íŠ¹ì„±ì„ ì´ìš©í•´ ì¡°ê¸ˆì”© ë‹¤ë¥¸ ëª¨ë¸ë“¤ì„ ë§Œë“¤ê³  ê·¸ê²ƒë“¤ì˜ ê²°ê³¼ë¥¼ ì·¨í•©í•˜ëŠ” ê²ƒì´ë‹¤.

ë°ì´í„°ê°€ ë¹„ìŠ·í•˜ë‹ˆ ê²°ê³¼ë„ ë³„ ì°¨ì´ê°€ ì—†ë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆì§€ë§Œ ë°ì´í„°ê°€ ë¹¼ê³¡í•´ì§€ëŠ” íš¨ê³¼ê°€ ìˆê³ , ì¡°ê¸ˆì”© ë‹¤ë¥¸ ëª¨ë¸ë“¤ì´ íˆ¬í‘œë¥¼ í•˜ëŠ” ë°©ì‹ìœ¼ë¯€ë¡œ ë°°ê¹…ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ì•™ìƒë¸” ëª¨ë¸ì€ ê²°ê³¼ë“¤ì— ëŒ€í•œ í¸ì°¨ê°€ í¬ì§€ ì•Šê³  ì•ˆì •ì ì¸ ê²°ê³¼ë¥¼ ë³´ì—¬ì§€ë„ë¡ í–¥ìƒë¨.

í•™ìŠµë°ì´í„°ê°€ ì›ë˜ í¸í–¥ì´ ìˆë‹¤ë©´ ê·¸ë¡œ ì¸í•œ í¸í–¥ë¬¸ì œê¹Œì§€ëŠ” í•´ê²°í•˜ì§€ ëª»í•˜ì§€ë§Œ ë¯¸ì§€ì˜ ë°ì´í„°(Unseen data)ì— ìƒë‹¹íˆ ê´œì°®ì€ ì„±ëŠ¥ì„ ë³´ì´ê³  ë…¸ì´ì¦ˆë‚˜ ì•„ì›ƒë¼ì´ì–´ì— ëŒ€í•´ì„œë„ ê°•í•´ì§€ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŒ.

ì¶œì²˜ : https://eunsukimme.github.io/ml/2019/11/26/Random-Forest/

ì´ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ë“¤ì€ ëª¨ë‘ ê°™ì€ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ì¼ ìˆ˜ê°€ ì—†ë‹¤

Bootstrapìœ¼ë¡œ ìƒ˜í”Œì„ ì¶”ì¶œí–ˆê¸° ë•Œë¬¸ì—, ì–´ëŠ ì •ë„ì˜ ì¤‘ë³µê³¼ í•¨ê»˜ ê°ê¸° ë‹¤ë¥¸ ìƒ˜í”Œë¡œ ì˜ì‚¬ ê²°ì •ë‚˜ë¬´ ëª¨ë¸ì„ ë§Œë“¤ì—ˆë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.

Hyperparameter
n_estimators: ëœë¤ í¬ë ˆìŠ¤íŠ¸ ì•ˆì˜ ê²°ì • íŠ¸ë¦¬ ê°¯ìˆ˜

n_estimatorsëŠ” í´ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤.
ê²°ì • íŠ¸ë¦¬ê°€ ë§ì„ìˆ˜ë¡ ë” ê¹”ë”í•œ Decision Boundaryê°€ ë‚˜ì˜¤ê² ì£ .
í•˜ì§€ë§Œ ê·¸ë§Œí¼ ë©”ëª¨ë¦¬ì™€ í›ˆë ¨ ì‹œê°„ì´ ì¦ê°€í•©ë‹ˆë‹¤. DefaultëŠ” 10ì…ë‹ˆë‹¤.
max_features: ë¬´ì‘ìœ„ë¡œ ì„ íƒí•  Featureì˜ ê°œìˆ˜
max_features=n_featuresì´ë©´ 30ê°œì˜ feature ì¤‘ 30ê°œì˜ feature ëª¨ë‘ë¥¼ ì„ íƒí•´ ê²°ì • íŠ¸ë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤. ë‹¨, bootstrap=Trueì´ë©´ 30ê°œì˜ featureì—ì„œ ë³µì› ì¶”ì¶œë¡œ 30ê°œë¥¼ ë½‘ìŠµë‹ˆë‹¤.

íŠ¹ì„± ì„ íƒì˜ ë¬´ì‘ìœ„ì„±ì´ ì—†ì–´ì§ˆ ë¿ ìƒ˜í”Œë§ì˜ ë¬´ì‘ìœ„ì„±ì€ ê·¸ëŒ€ë¡œì¸ ê²ƒì…ë‹ˆë‹¤.
bootstrap=TrueëŠ” default ê°’ì…ë‹ˆë‹¤.
ë”°ë¼ì„œ max_features ê°’ì´ í¬ë‹¤ë©´ ëœë¤ í¬ë ˆìŠ¤íŠ¸ì˜ íŠ¸ë¦¬ë“¤ì´ ë§¤ìš° ë¹„ìŠ·í•´ì§€ê³ , ê°€ì¥ ë‘ë“œëŸ¬ì§„ íŠ¹ì„±ì— ë§ê²Œ ì˜ˆì¸¡ì„ í•  ê²ƒì…ë‹ˆë‹¤.
max_features ê°’ì´ ì‘ë‹¤ë©´ ëœë¤ í¬ë ˆìŠ¤íŠ¸ì˜ íŠ¸ë¦¬ë“¤ì´ ì„œë¡œ ë§¤ìš° ë‹¬ë¼ì§ˆ ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì˜¤ë²„í”¼íŒ…ì´ ì¤„ì–´ë“¤ ê²ƒì…ë‹ˆë‹¤.
max_featuresëŠ” ì¼ë°˜ì ìœ¼ë¡œ Defalut ê°’ì„ ì”ë‹ˆë‹¤.
max_depth : íŠ¸ë¦¬ì˜ ê¹Šì´ë¥¼ ëœ»í•©ë‹ˆë‹¤.

min_samples_leaf : ë¦¬í”„ë…¸ë“œê°€ ë˜ê¸° ìœ„í•œ ìµœì†Œí•œì˜ ìƒ˜í”Œ ë°ì´í„° ìˆ˜ ì…ë‹ˆë‹¤.

min_samples_split : ë…¸ë“œë¥¼ ë¶„í• í•˜ê¸° ìœ„í•œ ìµœì†Œí•œì˜ ë°ì´í„° ìˆ˜ ì…ë‹ˆë‹¤.

max_leaf_nodes : ë¦¬í”„ë…¸ë“œì˜ ìµœëŒ€ ê°œìˆ˜

from sklearn.ensemble import RandomForestClassifier

from sklearn.datasets import make_moons

from sklearn.model_selection import train_test_split



x, y = make_moons(n_samples=100, noise=0.25, random_state=3) # noise: Standard deviation of Gaussian noise added to the data.



x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, random_state=42)



forest = RandomForestClassifier(n_estimators=5, n_jobs=-1, random_state=42) # n_estimators: ì‚¬ìš©í•  treeìˆ˜

forest.fit(x_train, y_train)



import matplotlib.pyplot as plt

import numpy as np

from mglearn.plots import plot_2d_classification



_, axes = plt.subplots(2, 3)

marker_set = ['o', '^']



for i, (axe, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):

    axe.set_title('tree {}'.format(i))

    plot_2d_classification(tree, x, fill=True, ax=axe, alpha=0.4)



    for i, m in zip(np.unique(y), marker_set):

        axe.scatter(x[y==i][:, 0], x[y==i][:, 1], marker=m,

                    label='class {}'.format(i), edgecolors='k')

        axe.set_xlabel('feature 0')

        axe.set_ylabel('feature 1')



axes[-1, -1].set_title('random forest')

axes[-1, -1].set_xlabel('feature 0')

axes[-1, -1].set_ylabel('feature 1')

plot_2d_classification(forest, x, fill=True, ax=axes[-1, -1], alpha=0.4)



for i, m in zip(np.unique(y), marker_set):

    plt.scatter(x[y==i][:, 0], x[y==i][:, 1], marker=m,

                label='class {}'.format(i), edgecolors='k')

plt.show()

#ì˜ì‚¬ê²°ì •ë‚˜ë¬´ Decision Tree (sklear.tree)

from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier 

X,y=make_moons(n_samples=100,noise=0.25,random_state=42)

X_train,X_test,y_train,y_test=train_test_split(X,y,stratify=y,random_state=42)

Decision=DecisionTreeClassifier(random_state=42)

Decision.fit(X_train,y_train)
print('Test Decision Tree Result score',Decision.score(X_test,y_test))

forest=RandomForestClassifier(n_estimators=5,n_jobs=-1,random_state=42) #CPU ëª‡ê°œ 

forest.fit(X_train,y_train)
print('Train RandomForest Result:', forest.score(X_train,y_train))

print('Test RandomForest:',forest.score(X_test,y_test)


#KWARGS: **kwargs parameter: ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°›ì•„ë“¤ì„

